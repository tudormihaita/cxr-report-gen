{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-05T19:51:12.060085Z",
     "start_time": "2025-04-05T19:51:10.304530Z"
    }
   },
   "source": [
    "import clip\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from loaders import CxrDataLoader\n",
    "from datasets import _build_iu_xray_sampler\n",
    "from classifier.model import ChexpertXRClassifier\n",
    "from classifier.trainer import ChexpertClassifierTrainer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:51:12.065249Z",
     "start_time": "2025-04-05T19:51:12.062858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LABELS = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture',\n",
    "          'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other', 'Pneumonia', 'Pneumothorax',\n",
    "          'Support Devices']\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    dataset_name='iu-xray',\n",
    "    batch_size=16,\n",
    "    num_workers=0,\n",
    "    max_seq_length=248\n",
    ")"
   ],
   "id": "e3aaf4f97807f79c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:51:13.328436Z",
     "start_time": "2025-04-05T19:51:12.115157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\", load_from_clip=True)\n",
    "clip_model.eval()\n",
    "\n",
    "vision_model = clip_model.visual\n",
    "input_dim = clip_model.vision_width\n",
    "hidden_dim = clip_model.transformer_width\n",
    "\n",
    "model = ChexpertXRClassifier(vision_model, input_dim, hidden_dim, num_classes=len(LABELS))"
   ],
   "id": "7542fa47b73efef9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:51:13.351364Z",
     "start_time": "2025-04-05T19:51:13.333576Z"
    }
   },
   "cell_type": "code",
   "source": "sampler = _build_iu_xray_sampler('train')",
   "id": "61a6ec695a73d215",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:51:13.382411Z",
     "start_time": "2025-04-05T19:51:13.357345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloaders = {\n",
    "    'train': CxrDataLoader(args, split='train', transform=preprocess, sampler=sampler),\n",
    "    'val': CxrDataLoader(args, split='val', transform=preprocess),\n",
    "    'test': CxrDataLoader(args, split='test', transform=preprocess)\n",
    "}"
   ],
   "id": "1f766edabe7a8423",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:51:13.394599Z",
     "start_time": "2025-04-05T19:51:13.388463Z"
    }
   },
   "cell_type": "code",
   "source": "trainer = ChexpertClassifierTrainer(model, train_loader=dataloaders['train'], val_loader=dataloaders['val'], log_interval=10)",
   "id": "3cc0f9a45d06e2e5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 22:51:13,392 - INFO - Total parameters: 88,250,126\n",
      "2025-04-05 22:51:13,393 - INFO - Trainable parameters: 88,250,126 (100.00%)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:18:29.602864Z",
     "start_time": "2025-04-05T19:51:13.459767Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(epochs=10)",
   "id": "b6d2997d54f23c2c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 22:51:13,460 - INFO - Start training\n",
      "2025-04-05 22:51:13,462 - INFO - Starting epoch 1/10\n",
      "  5%|▌         | 9/169 [00:11<03:31,  1.32s/it]2025-04-05 22:51:26,590 - INFO - Step 10: Loss = 0.3460\n",
      " 11%|█         | 19/169 [00:24<03:05,  1.23s/it]2025-04-05 22:51:39,179 - INFO - Step 20: Loss = 0.1695\n",
      " 17%|█▋        | 29/169 [00:37<03:01,  1.29s/it]2025-04-05 22:51:51,925 - INFO - Step 30: Loss = 0.2207\n",
      " 23%|██▎       | 39/169 [00:49<02:45,  1.27s/it]2025-04-05 22:52:04,602 - INFO - Step 40: Loss = 0.3786\n",
      " 29%|██▉       | 49/169 [01:02<02:30,  1.26s/it]2025-04-05 22:52:17,216 - INFO - Step 50: Loss = 0.3620\n",
      " 35%|███▍      | 59/169 [01:15<02:18,  1.26s/it]2025-04-05 22:52:29,779 - INFO - Step 60: Loss = 0.3945\n",
      " 41%|████      | 69/169 [01:27<02:07,  1.28s/it]2025-04-05 22:52:42,532 - INFO - Step 70: Loss = 0.4019\n",
      " 47%|████▋     | 79/169 [01:40<01:54,  1.28s/it]2025-04-05 22:52:55,335 - INFO - Step 80: Loss = 0.3331\n",
      " 53%|█████▎    | 89/169 [01:53<01:42,  1.28s/it]2025-04-05 22:53:08,013 - INFO - Step 90: Loss = 0.2994\n",
      " 59%|█████▊    | 99/169 [02:06<01:29,  1.27s/it]2025-04-05 22:53:20,879 - INFO - Step 100: Loss = 0.3323\n",
      " 64%|██████▍   | 109/169 [02:18<01:16,  1.28s/it]2025-04-05 22:53:33,585 - INFO - Step 110: Loss = 0.4454\n",
      " 70%|███████   | 119/169 [02:31<01:03,  1.27s/it]2025-04-05 22:53:46,375 - INFO - Step 120: Loss = 0.3567\n",
      " 76%|███████▋  | 129/169 [02:44<00:51,  1.29s/it]2025-04-05 22:53:59,540 - INFO - Step 130: Loss = 0.3917\n",
      " 82%|████████▏ | 139/169 [02:57<00:37,  1.26s/it]2025-04-05 22:54:12,319 - INFO - Step 140: Loss = 0.3889\n",
      " 88%|████████▊ | 149/169 [03:10<00:25,  1.29s/it]2025-04-05 22:54:25,177 - INFO - Step 150: Loss = 0.3489\n",
      " 94%|█████████▍| 159/169 [03:23<00:12,  1.30s/it]2025-04-05 22:54:38,078 - INFO - Step 160: Loss = 0.3572\n",
      "100%|██████████| 169/169 [03:35<00:00,  1.27s/it]\n",
      "2025-04-05 22:54:48,781 - INFO - Epoch 1 completed in 215.32s. Average loss: 0.3546\n",
      "2025-04-05 22:54:48,782 - INFO - Running evaluation on val set\n",
      "Val Evaluation: 100%|██████████| 37/37 [00:31<00:00,  1.19it/s]\n",
      "2025-04-05 22:55:19,822 - INFO - Number of uncertain cases predicted near 0.5: 0\n",
      "2025-04-05 22:55:19,856 - INFO - Val metrics: {\n",
      "  \"val_loss\": 0.22427096238007416,\n",
      "  \"val_auc\": 0.5323510433284181\n",
      "}\n",
      "2025-04-05 22:55:19,856 - INFO - Starting epoch 2/10\n",
      "  0%|          | 0/169 [00:00<?, ?it/s]2025-04-05 22:55:21,143 - INFO - Step 170: Loss = 0.3568\n",
      "  6%|▌         | 10/169 [00:12<03:22,  1.27s/it]2025-04-05 22:55:33,979 - INFO - Step 180: Loss = 0.3753\n",
      " 12%|█▏        | 20/169 [00:25<03:08,  1.26s/it]2025-04-05 22:55:46,780 - INFO - Step 190: Loss = 0.3568\n",
      " 18%|█▊        | 30/169 [00:38<02:58,  1.28s/it]2025-04-05 22:55:59,599 - INFO - Step 200: Loss = 0.4064\n",
      " 24%|██▎       | 40/169 [00:51<02:48,  1.31s/it]2025-04-05 22:56:12,486 - INFO - Step 210: Loss = 0.3187\n",
      " 30%|██▉       | 50/169 [01:03<02:27,  1.24s/it]2025-04-05 22:56:25,014 - INFO - Step 220: Loss = 0.3784\n",
      " 36%|███▌      | 60/169 [01:16<02:21,  1.29s/it]2025-04-05 22:56:37,881 - INFO - Step 230: Loss = 0.2606\n",
      " 41%|████▏     | 70/169 [01:29<02:07,  1.29s/it]2025-04-05 22:56:50,841 - INFO - Step 240: Loss = 0.3768\n",
      " 47%|████▋     | 80/169 [01:42<01:54,  1.29s/it]2025-04-05 22:57:03,620 - INFO - Step 250: Loss = 0.3678\n",
      " 53%|█████▎    | 90/169 [01:55<01:39,  1.27s/it]2025-04-05 22:57:16,551 - INFO - Step 260: Loss = 0.3871\n",
      " 59%|█████▉    | 100/169 [02:08<01:27,  1.27s/it]2025-04-05 22:57:29,239 - INFO - Step 270: Loss = 0.3974\n",
      " 65%|██████▌   | 110/169 [02:20<01:14,  1.26s/it]2025-04-05 22:57:41,844 - INFO - Step 280: Loss = 0.3105\n",
      " 71%|███████   | 120/169 [02:33<01:02,  1.28s/it]2025-04-05 22:57:54,792 - INFO - Step 290: Loss = 0.3534\n",
      " 77%|███████▋  | 130/169 [02:46<00:50,  1.29s/it]2025-04-05 22:58:07,516 - INFO - Step 300: Loss = 0.3632\n",
      " 83%|████████▎ | 140/169 [02:59<00:37,  1.28s/it]2025-04-05 22:58:20,243 - INFO - Step 310: Loss = 0.3320\n",
      " 89%|████████▉ | 150/169 [03:11<00:24,  1.27s/it]2025-04-05 22:58:33,071 - INFO - Step 320: Loss = 0.2671\n",
      " 95%|█████████▍| 160/169 [03:24<00:11,  1.29s/it]2025-04-05 22:58:45,809 - INFO - Step 330: Loss = 0.4879\n",
      "100%|██████████| 169/169 [03:35<00:00,  1.28s/it]\n",
      "2025-04-05 22:58:55,377 - INFO - Epoch 2 completed in 215.51s. Average loss: 0.3411\n",
      "2025-04-05 22:58:55,377 - INFO - Running evaluation on val set\n",
      "Val Evaluation: 100%|██████████| 37/37 [00:30<00:00,  1.21it/s]\n",
      "2025-04-05 22:59:26,007 - INFO - Number of uncertain cases predicted near 0.5: 60\n",
      "2025-04-05 22:59:26,040 - INFO - Val metrics: {\n",
      "  \"val_loss\": 0.2457557761588612,\n",
      "  \"val_best_threshold\": 0.4,\n",
      "  \"val_mean_f1\": 0.011614401858304297,\n",
      "  \"val_mean_precision\": 0.008025682182985553,\n",
      "  \"val_mean_recall\": 0.02100840336134454,\n",
      "  \"val_auc\": 0.5169130087801109\n",
      "}\n",
      "2025-04-05 22:59:26,040 - INFO - Starting epoch 3/10\n",
      "  1%|          | 1/169 [00:01<03:41,  1.32s/it]2025-04-05 22:59:28,676 - INFO - Step 340: Loss = 0.3446\n",
      "  7%|▋         | 11/169 [00:14<03:23,  1.29s/it]2025-04-05 22:59:41,661 - INFO - Step 350: Loss = 0.3062\n",
      " 12%|█▏        | 21/169 [00:27<03:08,  1.27s/it]2025-04-05 22:59:54,396 - INFO - Step 360: Loss = 0.3260\n",
      " 18%|█▊        | 31/169 [00:39<02:53,  1.25s/it]2025-04-05 23:00:06,974 - INFO - Step 370: Loss = 0.4089\n",
      " 24%|██▍       | 41/169 [00:52<02:45,  1.29s/it]2025-04-05 23:00:19,748 - INFO - Step 380: Loss = 0.3760\n",
      " 30%|███       | 51/169 [01:05<02:31,  1.28s/it]2025-04-05 23:00:32,409 - INFO - Step 390: Loss = 0.3517\n",
      " 36%|███▌      | 61/169 [01:17<02:16,  1.27s/it]2025-04-05 23:00:45,065 - INFO - Step 400: Loss = 0.3547\n",
      " 42%|████▏     | 71/169 [01:30<02:04,  1.27s/it]2025-04-05 23:00:57,779 - INFO - Step 410: Loss = 0.3806\n",
      " 48%|████▊     | 81/169 [01:43<01:53,  1.29s/it]2025-04-05 23:01:10,830 - INFO - Step 420: Loss = 0.0978\n",
      " 54%|█████▍    | 91/169 [01:56<01:38,  1.26s/it]2025-04-05 23:01:23,614 - INFO - Step 430: Loss = 0.3004\n",
      " 60%|█████▉    | 101/169 [02:09<01:28,  1.31s/it]2025-04-05 23:01:36,671 - INFO - Step 440: Loss = 0.3486\n",
      " 66%|██████▌   | 111/169 [02:22<01:14,  1.28s/it]2025-04-05 23:01:49,398 - INFO - Step 450: Loss = 0.5434\n",
      " 72%|███████▏  | 121/169 [02:34<00:59,  1.25s/it]2025-04-05 23:02:01,973 - INFO - Step 460: Loss = 0.3366\n",
      " 78%|███████▊  | 131/169 [02:47<00:48,  1.29s/it]2025-04-05 23:02:14,694 - INFO - Step 470: Loss = -0.0420\n",
      " 83%|████████▎ | 141/169 [03:00<00:36,  1.30s/it]2025-04-05 23:02:27,533 - INFO - Step 480: Loss = 0.3915\n",
      " 89%|████████▉ | 151/169 [03:13<00:23,  1.29s/it]2025-04-05 23:02:40,347 - INFO - Step 490: Loss = 0.5097\n",
      " 95%|█████████▌| 161/169 [03:25<00:10,  1.26s/it]2025-04-05 23:02:53,138 - INFO - Step 500: Loss = 0.0567\n",
      "100%|██████████| 169/169 [03:35<00:00,  1.27s/it]\n",
      "2025-04-05 23:03:01,406 - INFO - Epoch 3 completed in 215.35s. Average loss: 0.3429\n",
      "2025-04-05 23:03:01,407 - INFO - Running evaluation on val set\n",
      "Val Evaluation: 100%|██████████| 37/37 [00:30<00:00,  1.22it/s]\n",
      "2025-04-05 23:03:31,627 - INFO - Number of uncertain cases predicted near 0.5: 4\n",
      "2025-04-05 23:03:31,660 - INFO - Val metrics: {\n",
      "  \"val_loss\": 0.18192500585841165,\n",
      "  \"val_best_threshold\": 0.4,\n",
      "  \"val_mean_f1\": 0.018689581095596132,\n",
      "  \"val_mean_precision\": 0.010751359367276322,\n",
      "  \"val_mean_recall\": 0.07142857142857142,\n",
      "  \"val_auc\": 0.4450826092946489\n",
      "}\n",
      "2025-04-05 23:03:31,661 - INFO - Starting epoch 4/10\n",
      "  1%|          | 2/169 [00:02<03:44,  1.35s/it]2025-04-05 23:03:35,623 - INFO - Step 510: Loss = 0.3191\n",
      "  7%|▋         | 12/169 [00:15<03:20,  1.27s/it]2025-04-05 23:03:48,449 - INFO - Step 520: Loss = 0.4836\n",
      " 13%|█▎        | 22/169 [00:28<03:05,  1.26s/it]2025-04-05 23:04:01,101 - INFO - Step 530: Loss = 0.2028\n",
      " 19%|█▉        | 32/169 [00:40<02:54,  1.27s/it]2025-04-05 23:04:13,841 - INFO - Step 540: Loss = 0.4304\n",
      " 25%|██▍       | 42/169 [00:53<02:41,  1.27s/it]2025-04-05 23:04:26,618 - INFO - Step 550: Loss = 0.5325\n",
      " 31%|███       | 52/169 [01:06<02:29,  1.28s/it]2025-04-05 23:04:39,397 - INFO - Step 560: Loss = 0.3883\n",
      " 37%|███▋      | 62/169 [01:19<02:18,  1.29s/it]2025-04-05 23:04:52,322 - INFO - Step 570: Loss = 0.2984\n",
      " 43%|████▎     | 72/169 [01:32<02:03,  1.28s/it]2025-04-05 23:05:04,966 - INFO - Step 580: Loss = 0.4355\n",
      " 49%|████▊     | 82/169 [01:44<01:50,  1.27s/it]2025-04-05 23:05:17,636 - INFO - Step 590: Loss = 0.3311\n",
      " 54%|█████▍    | 92/169 [01:57<01:37,  1.27s/it]2025-04-05 23:05:30,414 - INFO - Step 600: Loss = 0.3165\n",
      " 60%|██████    | 102/169 [02:10<01:26,  1.29s/it]2025-04-05 23:05:43,283 - INFO - Step 610: Loss = 0.2914\n",
      " 66%|██████▋   | 112/169 [02:23<01:13,  1.29s/it]2025-04-05 23:05:56,108 - INFO - Step 620: Loss = 0.4625\n",
      " 72%|███████▏  | 122/169 [02:35<01:00,  1.28s/it]2025-04-05 23:06:08,970 - INFO - Step 630: Loss = 0.3870\n",
      " 78%|███████▊  | 132/169 [02:48<00:47,  1.29s/it]2025-04-05 23:06:21,791 - INFO - Step 640: Loss = 0.3398\n",
      " 84%|████████▍ | 142/169 [03:01<00:34,  1.28s/it]2025-04-05 23:06:34,572 - INFO - Step 650: Loss = 0.3745\n",
      " 90%|████████▉ | 152/169 [03:14<00:21,  1.27s/it]2025-04-05 23:06:47,249 - INFO - Step 660: Loss = 0.4314\n",
      " 96%|█████████▌| 162/169 [03:27<00:08,  1.28s/it]2025-04-05 23:07:00,205 - INFO - Step 670: Loss = 0.3482\n",
      "100%|██████████| 169/169 [03:35<00:00,  1.28s/it]\n",
      "2025-04-05 23:07:07,167 - INFO - Epoch 4 completed in 215.49s. Average loss: 0.3516\n",
      "2025-04-05 23:07:07,167 - INFO - Running evaluation on val set\n",
      "Val Evaluation: 100%|██████████| 37/37 [00:30<00:00,  1.20it/s]\n",
      "2025-04-05 23:07:38,043 - INFO - Number of uncertain cases predicted near 0.5: 0\n",
      "2025-04-05 23:07:38,076 - INFO - Val metrics: {\n",
      "  \"val_loss\": 0.2461469483536643,\n",
      "  \"val_best_threshold\": 0.4,\n",
      "  \"val_mean_f1\": 0.017968056787932563,\n",
      "  \"val_mean_precision\": 0.010387278789433187,\n",
      "  \"val_mean_recall\": 0.0665024630541872,\n",
      "  \"val_auc\": 0.4549710391219294\n",
      "}\n",
      "2025-04-05 23:07:38,077 - INFO - Starting epoch 5/10\n",
      "  2%|▏         | 3/169 [00:03<03:27,  1.25s/it]2025-04-05 23:07:43,129 - INFO - Step 680: Loss = 0.3642\n",
      "  8%|▊         | 13/169 [00:16<03:20,  1.28s/it]2025-04-05 23:07:56,008 - INFO - Step 690: Loss = 0.3986\n",
      " 14%|█▎        | 23/169 [00:29<03:03,  1.26s/it]2025-04-05 23:08:08,579 - INFO - Step 700: Loss = 0.3361\n",
      " 20%|█▉        | 33/169 [00:41<02:51,  1.26s/it]2025-04-05 23:08:21,331 - INFO - Step 710: Loss = 0.3613\n",
      " 25%|██▌       | 43/169 [00:54<02:41,  1.28s/it]2025-04-05 23:08:34,129 - INFO - Step 720: Loss = 0.1906\n",
      " 31%|███▏      | 53/169 [01:07<02:28,  1.28s/it]2025-04-05 23:08:46,850 - INFO - Step 730: Loss = 0.3785\n",
      " 37%|███▋      | 63/169 [01:20<02:12,  1.25s/it]2025-04-05 23:08:59,424 - INFO - Step 740: Loss = 0.3494\n",
      " 43%|████▎     | 73/169 [01:32<02:01,  1.27s/it]2025-04-05 23:09:12,023 - INFO - Step 750: Loss = 0.3648\n",
      " 49%|████▉     | 83/169 [01:45<01:48,  1.27s/it]2025-04-05 23:09:24,734 - INFO - Step 760: Loss = 0.3651\n",
      " 55%|█████▌    | 93/169 [01:58<01:37,  1.29s/it]2025-04-05 23:09:37,527 - INFO - Step 770: Loss = 0.3087\n",
      " 61%|██████    | 103/169 [02:10<01:25,  1.30s/it]2025-04-05 23:09:50,373 - INFO - Step 780: Loss = 0.3158\n",
      " 67%|██████▋   | 113/169 [02:23<01:11,  1.27s/it]2025-04-05 23:10:03,013 - INFO - Step 790: Loss = 0.3465\n",
      " 73%|███████▎  | 123/169 [02:36<00:58,  1.27s/it]2025-04-05 23:10:15,751 - INFO - Step 800: Loss = 0.3241\n",
      " 79%|███████▊  | 133/169 [02:49<00:45,  1.28s/it]2025-04-05 23:10:28,455 - INFO - Step 810: Loss = 0.2253\n",
      " 85%|████████▍ | 143/169 [03:01<00:32,  1.25s/it]2025-04-05 23:10:41,067 - INFO - Step 820: Loss = 0.2760\n",
      " 91%|█████████ | 153/169 [03:14<00:20,  1.26s/it]2025-04-05 23:10:53,952 - INFO - Step 830: Loss = 0.3280\n",
      " 96%|█████████▋| 163/169 [03:27<00:07,  1.28s/it]2025-04-05 23:11:06,818 - INFO - Step 840: Loss = 0.3287\n",
      "100%|██████████| 169/169 [03:34<00:00,  1.27s/it]\n",
      "2025-04-05 23:11:12,677 - INFO - Epoch 5 completed in 214.59s. Average loss: 0.3409\n",
      "2025-04-05 23:11:12,677 - INFO - Running evaluation on val set\n",
      "Val Evaluation: 100%|██████████| 37/37 [00:30<00:00,  1.20it/s]\n",
      "2025-04-05 23:11:43,421 - INFO - Number of uncertain cases predicted near 0.5: 0\n",
      "2025-04-05 23:11:43,452 - INFO - Val metrics: {\n",
      "  \"val_loss\": 0.23856385918082418,\n",
      "  \"val_auc\": 0.5497695136613607\n",
      "}\n",
      "2025-04-05 23:11:43,453 - INFO - Starting epoch 6/10\n",
      "  2%|▏         | 4/169 [00:05<03:32,  1.29s/it]2025-04-05 23:11:49,869 - INFO - Step 850: Loss = 0.3398\n",
      "  8%|▊         | 14/169 [00:17<03:17,  1.27s/it]2025-04-05 23:12:02,633 - INFO - Step 860: Loss = 0.4058\n",
      " 14%|█▍        | 24/169 [00:30<03:02,  1.26s/it]2025-04-05 23:12:15,198 - INFO - Step 870: Loss = 0.3465\n",
      " 20%|██        | 34/169 [00:43<02:50,  1.26s/it]2025-04-05 23:12:27,821 - INFO - Step 880: Loss = 0.2909\n",
      " 26%|██▌       | 44/169 [00:55<02:43,  1.31s/it]2025-04-05 23:12:40,712 - INFO - Step 890: Loss = 0.3917\n",
      " 32%|███▏      | 54/169 [01:08<02:23,  1.24s/it]2025-04-05 23:12:53,323 - INFO - Step 900: Loss = 0.3139\n",
      " 38%|███▊      | 64/169 [01:21<02:12,  1.26s/it]2025-04-05 23:13:05,940 - INFO - Step 910: Loss = 0.4124\n",
      " 44%|████▍     | 74/169 [01:34<02:01,  1.28s/it]2025-04-05 23:13:18,778 - INFO - Step 920: Loss = 0.3012\n",
      " 50%|████▉     | 84/169 [01:46<01:49,  1.29s/it]2025-04-05 23:13:31,584 - INFO - Step 930: Loss = 0.3804\n",
      " 56%|█████▌    | 94/169 [01:59<01:38,  1.31s/it]2025-04-05 23:13:44,746 - INFO - Step 940: Loss = 0.3358\n",
      " 62%|██████▏   | 104/169 [02:13<01:25,  1.32s/it]2025-04-05 23:13:57,916 - INFO - Step 950: Loss = 0.3334\n",
      " 67%|██████▋   | 114/169 [02:26<01:09,  1.27s/it]2025-04-05 23:14:10,868 - INFO - Step 960: Loss = 0.3606\n",
      " 73%|███████▎  | 124/169 [02:38<00:57,  1.27s/it]2025-04-05 23:14:23,621 - INFO - Step 970: Loss = 0.2856\n",
      " 79%|███████▉  | 134/169 [02:51<00:44,  1.27s/it]2025-04-05 23:14:36,364 - INFO - Step 980: Loss = 0.2966\n",
      " 85%|████████▌ | 144/169 [03:04<00:31,  1.26s/it]2025-04-05 23:14:49,140 - INFO - Step 990: Loss = 0.3212\n",
      " 91%|█████████ | 154/169 [03:17<00:18,  1.25s/it]2025-04-05 23:15:01,750 - INFO - Step 1000: Loss = 0.3024\n",
      " 97%|█████████▋| 164/169 [03:29<00:06,  1.26s/it]2025-04-05 23:15:14,400 - INFO - Step 1010: Loss = 0.2746\n",
      "100%|██████████| 169/169 [03:35<00:00,  1.27s/it]\n",
      "2025-04-05 23:15:18,818 - INFO - Epoch 6 completed in 215.35s. Average loss: 0.3408\n",
      "2025-04-05 23:15:18,818 - INFO - Running evaluation on val set\n",
      "Val Evaluation: 100%|██████████| 37/37 [00:30<00:00,  1.20it/s]\n",
      "2025-04-05 23:15:49,701 - INFO - Number of uncertain cases predicted near 0.5: 0\n",
      "2025-04-05 23:15:49,733 - INFO - Val metrics: {\n",
      "  \"val_loss\": 0.225991469984119,\n",
      "  \"val_auc\": 0.49050788364409137\n",
      "}\n",
      "2025-04-05 23:15:49,734 - INFO - Starting epoch 7/10\n",
      "  3%|▎         | 5/169 [00:06<03:27,  1.27s/it]2025-04-05 23:15:57,394 - INFO - Step 1020: Loss = 0.3847\n",
      "  9%|▉         | 15/169 [00:19<03:18,  1.29s/it]2025-04-05 23:16:10,307 - INFO - Step 1030: Loss = 0.3076\n",
      " 15%|█▍        | 25/169 [00:32<03:04,  1.28s/it]2025-04-05 23:16:23,030 - INFO - Step 1040: Loss = 0.2819\n",
      " 21%|██        | 35/169 [00:44<02:48,  1.26s/it]2025-04-05 23:16:35,563 - INFO - Step 1050: Loss = 0.3300\n",
      " 27%|██▋       | 45/169 [00:57<02:34,  1.25s/it]2025-04-05 23:16:48,211 - INFO - Step 1060: Loss = 0.3638\n",
      " 33%|███▎      | 55/169 [01:10<02:28,  1.31s/it]2025-04-05 23:17:01,144 - INFO - Step 1070: Loss = 0.3200\n",
      " 38%|███▊      | 65/169 [01:22<02:12,  1.27s/it]2025-04-05 23:17:13,839 - INFO - Step 1080: Loss = 0.3211\n",
      " 44%|████▍     | 75/169 [01:35<02:00,  1.29s/it]2025-04-05 23:17:26,478 - INFO - Step 1090: Loss = 0.3901\n",
      " 50%|█████     | 85/169 [01:48<01:49,  1.31s/it]2025-04-05 23:17:39,463 - INFO - Step 1100: Loss = 0.4009\n",
      " 56%|█████▌    | 95/169 [02:01<01:37,  1.32s/it]2025-04-05 23:17:52,713 - INFO - Step 1110: Loss = 0.2492\n",
      " 62%|██████▏   | 105/169 [02:14<01:22,  1.28s/it]2025-04-05 23:18:05,713 - INFO - Step 1120: Loss = 0.3293\n",
      " 68%|██████▊   | 115/169 [02:27<01:07,  1.26s/it]2025-04-05 23:18:18,337 - INFO - Step 1130: Loss = 0.3283\n",
      " 73%|███████▎  | 124/169 [02:39<00:57,  1.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/classifier/trainer.py:126\u001B[0m, in \u001B[0;36mChexpertClassifierTrainer.train\u001B[0;34m(self, epochs)\u001B[0m\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReached maximum steps \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_steps\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Stopping training.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m--> 126\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/classifier/trainer.py:150\u001B[0m, in \u001B[0;36mChexpertClassifierTrainer.train_step\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m--> 150\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    152\u001B[0m logits \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogits\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    153\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn(logits, labels)\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/classifier/model.py:35\u001B[0m, in \u001B[0;36mChexpertXRClassifier.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 35\u001B[0m     _, x_embed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvision_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mforward(x_embed)\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/clip/model.py:175\u001B[0m, in \u001B[0;36mVisionTransformer.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    172\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_pre(x)\n\u001B[1;32m    174\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# NLD -> LND: permute to match the transformer's input shape (batch_size, num_patches, embedding_dim)\u001B[39;00m\n\u001B[0;32m--> 175\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# apply the transformer\u001B[39;00m\n\u001B[1;32m    176\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# permute back to the original shape\u001B[39;00m\n\u001B[1;32m    178\u001B[0m x_cls \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_post(x[:, \u001B[38;5;241m0\u001B[39m, :])  \u001B[38;5;66;03m# apply layer normalization to the CLS token\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/clip/model.py:128\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresblocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/clip/model.py:115\u001B[0m, in \u001B[0;36mResidualAttentionBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;66;03m# follows the Pre-LN transformer design, where the layer normalization is applied before the attention mechanism\u001B[39;00m\n\u001B[0;32m--> 115\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mln_1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_2(x))\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/clip/model.py:111\u001B[0m, in \u001B[0;36mResidualAttentionBlock.attention\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mattention\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_mask\u001B[38;5;241m.\u001B[39mto(dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/modules/activation.py:1373\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[1;32m   1347\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[1;32m   1348\u001B[0m         query,\n\u001B[1;32m   1349\u001B[0m         key,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1370\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal,\n\u001B[1;32m   1371\u001B[0m     )\n\u001B[1;32m   1372\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1373\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1374\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1375\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1376\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1377\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1378\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1379\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1380\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1381\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1382\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1383\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1384\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1385\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1386\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1388\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1389\u001B[0m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1390\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1391\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1392\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1393\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[1;32m   1395\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/functional.py:6230\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[0m\n\u001B[1;32m   6226\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m use_separate_proj_weight:\n\u001B[1;32m   6227\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m   6228\u001B[0m         in_proj_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   6229\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 6230\u001B[0m     q, k, v \u001B[38;5;241m=\u001B[39m \u001B[43m_in_projection_packed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_proj_bias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   6232\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m   6233\u001B[0m         q_proj_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   6234\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/clip-cxr-report-gen/.venv/lib/python3.9/site-packages/torch/nn/functional.py:5614\u001B[0m, in \u001B[0;36m_in_projection_packed\u001B[0;34m(q, k, v, w, b)\u001B[0m\n\u001B[1;32m   5611\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mis\u001B[39;00m v:\n\u001B[1;32m   5612\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m q \u001B[38;5;129;01mis\u001B[39;00m k:\n\u001B[1;32m   5613\u001B[0m         \u001B[38;5;66;03m# self-attention\u001B[39;00m\n\u001B[0;32m-> 5614\u001B[0m         proj \u001B[38;5;241m=\u001B[39m \u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5615\u001B[0m         \u001B[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001B[39;00m\n\u001B[1;32m   5616\u001B[0m         proj \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   5617\u001B[0m             proj\u001B[38;5;241m.\u001B[39munflatten(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, (\u001B[38;5;241m3\u001B[39m, E))\n\u001B[1;32m   5618\u001B[0m             \u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5621\u001B[0m             \u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m   5622\u001B[0m         )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:18:31.648464Z",
     "start_time": "2025-04-05T20:18:30.924822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loader = dataloaders['test']\n",
    "test_loader_iter = iter(test_loader)\n",
    "batch = next(test_loader_iter)\n",
    "\n",
    "labels = batch['labels'][0]\n",
    "labels[labels == -1] = 0.0\n",
    "image = batch['image'][0]\n",
    "image = image.unsqueeze(0)\n",
    "predictions = model(image)\n",
    "\n",
    "print(predictions['probs'])\n",
    "print(labels)"
   ],
   "id": "3886af1f3999249a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0911, 0.2139, 0.0697, 0.0712, 0.0050, 0.1004, 0.0641, 0.3036, 0.0780,\n",
      "         0.1526, 0.0532, 0.0622, 0.1173, 0.1548]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
